<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Deep Learning | Khush: Roboticist</title>
    <link>https://khush3.github.io/tags/deep-learning/</link>
      <atom:link href="https://khush3.github.io/tags/deep-learning/index.xml" rel="self" type="application/rss+xml" />
    <description>Deep Learning</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Copyright © 2020 Khush Agrawal</copyright><lastBuildDate>Wed, 01 Jan 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://khush3.github.io/img/roboticist.jpg</url>
      <title>Deep Learning</title>
      <link>https://khush3.github.io/tags/deep-learning/</link>
    </image>
    
    <item>
      <title>Experience Transfer</title>
      <link>https://khush3.github.io/project/experience-transfer/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://khush3.github.io/project/experience-transfer/</guid>
      <description>&lt;p&gt;•Aimed to transfer the experience of a teacher agent, receiving higher and lower dimensional observations to train student-agent, receiving only higher dimensional observations.&lt;/p&gt;
&lt;h3 id=&#34;abstract&#34;&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Learning directly from higher dimensional data like video stream is known to be a difficult problem to tackle in Reinforcement Learning. Learning directly from higher dimensional data can also be very time consuming. Imitation Learning can be used in such cases to avoid random policy initializations. However, to use Imitation Learning, one needs to generate experience from an (expert) agent. A human (expert) agent generating these experience, needs to follow a set of ground rules to stick to the IID-data assumption needed to ensure stability in training.&lt;br&gt;
One method to avoid the cumbersome process of setting the ground rules could be to use an (expert) agent, trained on lower dimensional observations. Training on lower dimensional data is known to be computationally efficient and less time consuming&lt;!--[Citation needed] --&gt;. The experience of this trained agent can hence be used to train the higher dimensional agent.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>RL on Latent Embeddings</title>
      <link>https://khush3.github.io/project/representation_rl/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://khush3.github.io/project/representation_rl/</guid>
      <description>&lt;p&gt;•Aimed at training Deep Q-Learning based agents on learned latent embeddings instead of higher dimensional image observations.&lt;/p&gt;
&lt;h3 id=&#34;abstract&#34;&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Learning directly from higher dimensional data like video stream is known to be a difficult problem to tackle in Reinforcement Learning. Learning directly from higher dimensional data can also be very time consuming. Representation learning can be used in such cases to leverage lower dimensional trained agents. The two-stage agent can be further be fine-tuned to adapt better using end-to-end training. This approach is also more robust to variations in environment due to the fact that agents are trained on learnt representations instead of directly training on the environment observations.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Implementation of RL Algorithms</title>
      <link>https://khush3.github.io/project/rl-implementation/</link>
      <pubDate>Sat, 01 Jun 2019 00:00:00 +0000</pubDate>
      <guid>https://khush3.github.io/project/rl-implementation/</guid>
      <description>&lt;h3 id=&#34;abstract&#34;&gt;Abstract&lt;/h3&gt;
&lt;p&gt;To broaden my perspective on Machine Learning, I took up Reinforcement Learning courses by David Silver, Stanford CS234. To understand the nuances in the field, I implemented basic algorithms. The inspiration for the same was also drawn through 
&lt;a href=&#34;https://khush3.github.io/publication/stair_climbing/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;a project&lt;/a&gt;
 where the idea of using Imitation Learning to overcome several limitations, struck me.&lt;/p&gt;
&lt;h3 id=&#34;implementated&#34;&gt;Implementated:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt;DQN&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt;Vanilla Policy Gradient&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt;PPO&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt;DDPG&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;results&#34;&gt;Results&lt;/h3&gt;
&lt;h4 id=&#34;value-and-policy-iteration&#34;&gt;Value and Policy Iteration&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;https://media.giphy.com/media/L05KQuhnujAW3QyIkG/giphy.gif&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;Method&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Deterministic Frozen Lake&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Stochastic Frozen Lake&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;Value Iteration&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;7&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;8&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;Policy Iteration&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;7&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;3&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4 id=&#34;q-learning&#34;&gt;Q-Learning&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;https://media.giphy.com/media/KyAYvfmKlabE1zC820/giphy.gif&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h4 id=&#34;deep-q-learning&#34;&gt;Deep Q-Learning&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;https://media.giphy.com/media/PlsqPhHU7KnB2AdmYa/giphy.gif&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
