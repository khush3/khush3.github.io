[{"authors":["admin"],"categories":null,"content":"I am a graduate student at the Robotics Institute, Carnegie Mellon University (CMU). I am interested in working at the intersection of machine learning (ML) and robotics.\nBefore joining CMU, I worked as a Research Associate at Prof Howie Choset's Biorobotics Lab on skill learning and snake robots. During my undergrad, I spent time working at Dr. Shital Chiddarwar's IvLabs, building a stair climbing robot and an assistive robot. I spent the summer of 2020 working in Dr. David Held's R-PAD Lab as a part of the Robotics Institute Summer Scholar\u0026rsquo;s Program remotely.\nBesides research, I enjoy music, biking, and spending time in nature.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://khush3.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am a graduate student at the Robotics Institute, Carnegie Mellon University (CMU). I am interested in working at the intersection of machine learning (ML) and robotics.\nBefore joining CMU, I worked as a Research Associate at Prof Howie Choset's Biorobotics Lab on skill learning and snake robots. During my undergrad, I spent time working at Dr. Shital Chiddarwar's IvLabs, building a stair climbing robot and an assistive robot. I spent the summer of 2020 working in Dr.","tags":null,"title":"Khush Agrawal","type":"authors"},{"authors":null,"categories":null,"content":"Abstract Agents building temporally abstract representations of their environment can better understand their world and make plans on extended time scales with limited computational power and modeling capacity. Existing methods for learning world models either operate per timestep or assume a fixed number of timesteps for abstraction. Our approach simultaneously learns variable-length skills and temporally abstract, skill-conditioned world models from offline data. This leads to much higher confidence in dynamics predictions allowing zero-shot online planning by composing skills for new tasks. Furthermore, compared to policy-based methods, this approach offers a much higher degree of robustness to perturbations in environmental dynamics.\nOverview of offline skill learning During the offline training phase, our algorithm automatically extracts semantically meaningful skills and skill conditioned dynamics model from the offline data. During the online planning phase, the planner uses the learned skill conditioned dynamics model to plan a sequence of skills to achieve the goal.\nLegend Notation Environment state: $s$ Action: $a$ trajectory: $\\tau$ skill: $z$ Components Skill prior: $z \\sim p_{\\omega}(z | s_0)$ Abstract dynamics model: $s_T \\sim p_{\\psi}(s_T | s_0, z)$ Lower level policy: $a_t \\sim \\pi_{\\theta}(a_t | s_t, z)$ Termination predictor: $b_t = p_{\\phi}(s_t, z, s_T) \u0026gt; 0.5$ Results Planning procedure During the offline skill extraction phase, we learn a state conditioned prior ($p_{\\omega}$) that outputs a priori skill distribution given environment state ($p_{\\omega}(z|s_t)$). We use this to sample a batch of skills. These sampled skills are inputed to the abstract dynamics model ($p_{\\psi}$) that predicts the terminal state distribution for each of the skill sample ($s_T \\sim p_{\\psi}(s_T|s_0, z)$). Hence, we select a subset of the skills with terminal states closest to the goal state. Finally, we use the selected skills to execute in the environment. Execution of the selected skills For execution we pass the selected skill and the environment state through the lower level policy ($\\pi_{\\theta}$) that predicts the action to execute ($a_t \\sim \\pi_{\\theta}(a_t|s_t, z)$). The action is then executed in the environment and the next state is observed. This process is repeated until termination is predicted by the termination predictor ($b_t = 1 = p_{\\phi}(s_t, z, s_T) \u003e 0.5$). ","date":1680307200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1680307200,"objectID":"a4abf89b0f061da344ac5ae0e9dc4f82","permalink":"https://khush3.github.io/project/skill-learning/","publishdate":"2023-04-01T00:00:00Z","relpermalink":"/project/skill-learning/","section":"project","summary":"Skill (option) \u0026 skill-conditioned abstract world model extraction from offline data.","tags":["Reinforcement learning","Machine learning"],"title":"Skill learning","type":"project"},{"authors":null,"categories":null,"content":"Abstract SnakeLib is a ROS-based package that holds past gait research on modular snake robots and serves as a platform to facilitate further development. SnakeLibâ€™s basic gait implementations provide a user-friendly and intuitive way to operate the robot with a joystick.\nContributions A set of ROS packages for simulating and controlling Biorobotics Lab snake robots. ROS based software to control ReU, SEA, and RSnake snake robots, receive sensor feedback, and create visualizations. ROS diagram HEBI ros node for sending joint commands to HEBI electronics and reading sensor feedback Camera node to read camera in the ReU snakehead Gait library to host manually programmed snake gaits Joystick node to convert joystick inputs to snake commands Robot class for easy to use IK and FK implementations RViz and PyQt based GUI to visualize state and camera feedback Snake state node for virtual chassis implementation for robot state visualization GUI node to create PyQt interface Demos Sidewinding Summary $$ \\alpha(n, t) = \\beta_{even} + A_{even}sin(\\omega_{t, even}t + \\omega_{s, even}n) \\forall \\text{even n} $$ $$ \\alpha(n, t) = \\beta_{odd} + A_{odd}sin(\\omega_{t, odd} + \\omega_{s, odd}n) \\forall \\text{odd n} $$ Sideways moving gait inspired by snake's sidewinding motion. Rolling Summary $$ \\alpha(n, t) = \\beta_{even} + A_{even}sin(\\omega_{t, even}t + \\omega_{s, even}n) \\forall \\text{even n} $$ $$ \\alpha(n, t) = \\beta_{odd} + A_{odd}sin(\\omega_{t, odd} + \\omega_{s, odd}n) \\forall \\text{odd n} $$ Sideways moving gait inspired rolling motion of a body. Here, the wave only propogates in time (i.e., $\\omega_s = 0$) and the horizontal and vertical waves are offset by $\\pi/2$. Linear progression Summary $$ \\alpha(n, t) = \\beta_{even} + A_{even}sin(\\omega_{t, even}t + \\omega_{s, even}n) \\forall \\text{even n} $$ $$ \\alpha(n, t) = \\beta_{odd} + A_{odd}sin(\\omega_{t, odd} + \\omega_{s, odd}n) \\forall \\text{odd n} $$ Pole climbing Summary IK-based headlook Summary $$ \\dot{q} = J(q)^{+}v $$ Here, $\\dot{q}$: Joint velocities $J^{+}$: Pseudo-inverse of the jacobian $v$: End-effector velocity Controls the end-effector (snake head) in cartesian space using inverse jacobian approach. Slithering Summary Forward moving gait inspired by snake's [slithering](https://en.wikipedia.org/wiki/Snake_locomotion#Slithering) motion. Turn-in-place Summary $$ \\alpha(n, t) = \\beta_{even} + A_{even}sin(\\omega_{t, even}t + \\omega_{s, even}n) \\forall \\text{even n} $$ $$ \\alpha(n, t) = \\beta_{odd} + A_{odd}sin(\\omega_{t, odd} + \\omega_{s, odd}n) \\forall \\text{odd n} $$ Implemented by running opposite direction sidewinding gait in two halfs of the snake robot. ","date":1680307200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1680307200,"objectID":"e1d7e395f38600033e83a82dd549385b","permalink":"https://khush3.github.io/project/snake-robot/","publishdate":"2023-04-01T00:00:00Z","relpermalink":"/project/snake-robot/","section":"project","summary":"ROS packages for simulating and controlling biorobotics lab snake robots.","tags":["Electronics","Embedded systems","Robotics","ROS"],"title":"Snake-like robots","type":"project"},{"authors":["Thomas Weng","Sujay Bajracharya","Yufei Wang","**Khush Agrawal**","David Held"],"categories":null,"content":"","date":1636243200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636243200,"objectID":"ea6ee2e269d307e1e3e61c0fdc6d45b0","permalink":"https://khush3.github.io/publication/fabric_manipulation/","publishdate":"2021-11-07T00:00:00Z","relpermalink":"/publication/fabric_manipulation/","section":"publication","summary":"We address the problem of goal-directed cloth manipulation, a challenging task due to the deformability of cloth. Our insight is that optical flow, a technique normally used for motion estimation in video, can also provide an effective representation for corresponding cloth poses across observation and goal images. We introduce FabricFlowNet (FFN), a cloth manipulation policy that leverages flow as both an input and as an action representation to improve performance. FabricFlowNet also elegantly switches between bimanual and single-arm actions based on the desired goal. We show that FabricFlowNet significantly outperforms state-of-the-art model-free and model-based cloth manipulation policies that take image input. We also present real-world experiments on a bimanual system, demonstrating effective sim-to-real transfer. Finally, we show that our method generalizes when trained on a single square cloth to other cloth shapes, such as T-shirts and rectangular cloths. Video and other supplementary materials are available at: https://sites.google.com/view/fabricflownet.","tags":null,"title":"FabricFlowNet: Bimanual Cloth Manipulation with a Flow-based Policy","type":"publication"},{"authors":["**Khush Agrawal**","Rohit Lal","Himanshu Patil","Surender Kannaiyan","Deep Gupta"],"categories":null,"content":"DeepSCT Architecture An abstract block diagram of the proposed DeepSCT mechanism demonstrating the passage of data between the individual sub-blocks. The target is #rst acquired from the camera and fed into the Deep-SCT block for continuous tracking while also correcting compounded errors. This correction is achieved due to a continuous feedback loop mechanism using re-identification and detection sub-blocks. Note: Dotted line indicates a one-time initialization procedure.\nDeepSCT for long-horizon tracking and occlusion handling We adopted the standard method: success and precision plots to evaluate the performance of our mechanism against some of the standard classical computer vision baselines. We perform One Pass Evaluation (OPE) for our mechanism on the VisDrone-2019 test dataset [1]. We present the tests we conducted on our mechanism on a multitude of test cases covering variable lighting, camera orientation, and object sizes. We tested DeepSCT on the VisDrone-SOT2019 dataset for a person-tracking task. We show that DeepSCT consistently outperforms the classical trackers in short-term and long-term tracking problems. We also show that DeepSCT can handle occlusions better than the classical trackers.\nQuantitative results The success and precision plots for evaluating DeepSCT (indicated in blue) against the baseline classical CV algorithms in long-term sequences. DeepSCT scored notably higher AUC scores outperforming all of the baseline trackers. Compared to short-term tracking, DeepSCT provides a considerably higher improvement for long-term tracking. Furthermore, the improvement is significant when the threshold is reasonable.\nAirSim drone simulation Similar to the previous section, we adopt the standard success, and precision curves to evaluate the DeepSCT mechanism for the custom AirSim drone simulation environment. Accordingly, we perform One Pass Evaluation (OPE) by simulating ten trajectories while incorporating several instances of occlusion, viewpoint, and lighting changes. We also calculate the Area Under Curve (AUC) for both the plots for all tested algorithms and show these in the quantitative results. The plots clearly illustrate that the DeepSCT mechanism outperforms all of the classical algorithms by a significant margin. This significant difference in performance is a result of the inherent correcting nature of the DeepSCT mechanism. While other algorithms are unable to recover in cases of failure, DeepSCT can still recover. As a result, classical methods fail severely in long-term trajectories.\nQuantitative results The success and precision plots for comparing DeepSCT (indicated in blue) against the baseline classical computer vision algorithms. It is clear from the plots that DeepSCT outperforms the other trackers by a significant margin).\nReferences [1] Du, Dawei, Pengfei Zhu, Longyin Wen, Xiao Bian, Haibin Ling, Qinghua Hu, Jiayu Zheng et al. \u0026ldquo;VisDrone-SOT2019: The vision meets drone single object tracking challenge results.\u0026rdquo; In Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops, pp. 0-0. 2019.\n","date":1627344000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1627344000,"objectID":"c4729bf0eb9dd2a12ac220f160f4aee7","permalink":"https://khush3.github.io/publication/deep_sct/","publishdate":"2021-07-27T00:00:00Z","relpermalink":"/publication/deep_sct/","section":"publication","summary":"We present a novel mechanism, DeepSCT, to handle the long-term object tracking problem. We build around the premise that the classical tracking algorithms can handle short-term tracking problems efficiently; however, failing in long-term tracking due to occlusion and lost targets. On the other hand, deep learning object detection has higher efficacy but suffers from heavy computational requirements. We present a modular fusion mechanism that inherits higher efficiency and efficacy simultaneously. Finally, we showcase the significant speed and precision improvements in the VisDrone-SOT2019 dataset and an application of person following in a custom AirSim drone simulation.","tags":null,"title":"DeepSCT: Deep Learning Based Self Correcting Object Tracking Mechanism","type":"publication"},{"authors":["**Khush Agrawal**","Rohit Lal"],"categories":null,"content":"","date":1578614400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578614400,"objectID":"ec2c458087b334f51d51991473c6d7db","permalink":"https://khush3.github.io/publication/person_follower/","publishdate":"2020-01-10T00:00:00Z","relpermalink":"/publication/person_follower/","section":"publication","summary":"Assistive robots can augment the human experience, especially for the infirm and elderly. We demonstrate the use case of a person-following robot to carry a user's luggage. Using an amalgamation of classical computer vision (CSRT tracking) and deep learning (YOLO object detection) techniques, we develop a long-term person-tracking pipeline with intuitive gestures for robot control. Finally, we showcase the end-to-end pipeline on a TurtleBot2 in the physical world with various cases of occlusions and user variations.","tags":null,"title":"Person Following Robot using Multiplexed Detection and Tracking","type":"publication"},{"authors":null,"categories":null,"content":"Aimed to transfer the experience of a teacher agent, receiving higher and lower dimensional observations to train student-agent, receiving only higher dimensional observations.\nAbstract Learning directly from higher dimensional data like video stream is known to be a difficult problem to tackle in Reinforcement Learning. Learning directly from higher dimensional data can also be very time consuming. Imitation Learning can be used in such cases to avoid random policy initializations. However, to use Imitation Learning, one needs to generate experience from an (expert) agent. A human (expert) agent generating these experience, needs to follow a set of ground rules to stick to the IID-data assumption needed to ensure stability in training. One method to avoid the cumbersome process of setting the ground rules could be to use an (expert) agent, trained on lower dimensional observations. Training on lower dimensional data is known to be computationally efficient and less time consuming. The experience of this trained agent can hence be used to train the higher dimensional agent.\n","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"3254a4cb569580f365a33a4f5c66929b","permalink":"https://khush3.github.io/project/experience-transfer/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/project/experience-transfer/","section":"project","summary":"Framework to teach higher-dim student using experiences of a teacher.","tags":["Reinforcement learning","Deep learning"],"title":"Experience Transfer","type":"project"},{"authors":null,"categories":null,"content":"Aimed at training Deep Q-Learning based agents on learned latent embeddings instead of higher dimensional image observations.\nAbstract Learning directly from higher dimensional data like video stream is known to be a difficult problem to tackle in Reinforcement Learning. Learning directly from higher dimensional data can also be very time consuming. Representation learning can be used in such cases to leverage lower dimensional trained agents. The two-stage agent can be further be fine-tuned to adapt better using end-to-end training. This approach is also more robust to variations in environment due to the fact that agents are trained on learnt representations instead of directly training on the environment observations.\n","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"f660ef028d0f6c7e21471e38c1d45f26","permalink":"https://khush3.github.io/project/rl-for-rl/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/project/rl-for-rl/","section":"project","summary":"Framework to train agent using representation learning.","tags":["Reinforcement learning","Deep learning"],"title":"RL on Latent Embeddings","type":"project"},{"authors":["Navid Panchi","**Khush Agrawal**","Unmesh Patil","Aniket Gujarathi","Aman Jain","Harsha Namdeo","Shital S. Chiddarwar"],"categories":null,"content":"Qualitative results Stair segmentation We use a U-Net model for segmenting the stairs in the RGBD image input. Stair alignment We use behavior cloning to learn a model that predicts actions (turn left/right, move forwards/backwards) per time step to align the TurtleBot2 with the stairs, given a segmented image of the stairs. Quantitative results ","date":1575158400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575158400,"objectID":"183bd3bd75c692130b3f4998efcbbfc5","permalink":"https://khush3.github.io/publication/stair_climbing/","publishdate":"2019-12-01T00:00:00Z","relpermalink":"/publication/stair_climbing/","section":"publication","summary":"Mobile robots are widely used in the surveillance industry, for military and industrial applications. In order to carry out surveillance tasks like urban search and rescue operations, the ability to traverse stairs is of immense significance. This paper presents a deep learning based approach for semantic segmentation of stairs, behavioral cloning for star alignment, and a novel mechanical design for an autonomous stair climbing robot. The main objective is to solve the problem of locomotion over staircases with the proposed implementation. Alignment of a robot with stairs in an image is a traditional problem, and the most recent approaches are centred around hand-crafted texture-based Gabor filters and stair detection techniques. However, we could arrive at a more scalable and robust pipeline for alignment schemes. The proposed deep learning technique eliminates the need for manual tuning of parameters of the edge detector, the Hough accumulator, and PID constants. The empirical results and architecture of the stair alignment pipeline are demonstrated in this paper.","tags":null,"title":"Deep Learning-Based Stair Segmentation and Behavioral Cloning for Autonomous Stair Climbing","type":"publication"},{"authors":null,"categories":null,"content":" ","date":1559347200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559347200,"objectID":"9c8db0ef9ee77ac6a9843757c12ab58a","permalink":"https://khush3.github.io/mentorship/rl-basics/","publishdate":"2019-06-01T00:00:00Z","relpermalink":"/mentorship/rl-basics/","section":"mentorship","summary":"Mentored 5 sophomores for their first project in Reinforcement Learning.","tags":["Machine learning","Reinforcement learning"],"title":"Basics of Reinforcement Learning","type":"mentorship"},{"authors":null,"categories":null,"content":"Abstract Working on the autonomous stair-climbing robot highlighted the limitations of behavior cloning. This realization motivated me to study reinforcement learning. Following are some of the results of my implementation of basic reinforcement learning algorithms form scratch using PyTorch, NumPy, and OpenCV.\nImplementated: DQN Vanilla Policy Gradient PPO DDPG Results Value and Policy Iteration Method Deterministic Frozen Lake Stochastic Frozen Lake Value Iteration 7 8 Policy Iteration 7 3 Q-Learning Deep Q-Learning ","date":1559347200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559347200,"objectID":"14af0a19b7af35021d4397ca40c42784","permalink":"https://khush3.github.io/project/rl-basics/","publishdate":"2019-06-01T00:00:00Z","relpermalink":"/project/rl-basics/","section":"project","summary":"Implementation of basic reinforcement learning algorithms.","tags":["Reinforcement learning","Deep learning"],"title":"Implementation of RL Algorithms","type":"project"},{"authors":null,"categories":null,"content":"","date":1559347200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559347200,"objectID":"4fc20748aa56b50a71ac9e0047bf58a3","permalink":"https://khush3.github.io/mentorship/digit-classifiers/","publishdate":"2019-06-01T00:00:00Z","relpermalink":"/mentorship/digit-classifiers/","section":"mentorship","summary":"Mentored 9 freshmen for their first project in Machine Learning.","tags":["Machine learning"],"title":"Real Time Digit Classifier","type":"mentorship"},{"authors":null,"categories":null,"content":"Abstract Robots are widely used in manufacturing, assembly and packing, transport, earth and space exploration, surgery, weaponry, laboratory research, and mass production of consumer and industrial goods. A Robot equipped with a Pick-and-Place mechanism can be used for numerous applications. The gripper mechanism was created using two spur gears with one fitted to a servo and the other to a dead-axle. Four additional servos were used to create the robotic manipulator arm.\nContributions Aimed at developing a wirelessly controlled robot capable of moving objects. Developed a 3 DoF servo controlled arm with a gripper end effector for Pick-and-Place mechanism. Used 2 Arduinos for controlling arm and motion independently. Used a single channel relay for optimizing power consumption by switching off the arm when not in use. This project was presented for AXIS (Technical festival at NIT-Nagpur), 2017 and was awarded a prize for innovative design. ","date":1559347200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559347200,"objectID":"56b9041b26c5c32ed1b776df1f57a2af","permalink":"https://khush3.github.io/project/terra-former-robot/","publishdate":"2019-06-01T00:00:00Z","relpermalink":"/project/terra-former-robot/","section":"project","summary":"Differential Drive robot for moving objects.","tags":["Electronics","Embedded systems","Robotics"],"title":"Terra-former Robot","type":"project"},{"authors":null,"categories":null,"content":"Abstract Description Sign languages are languages that use the visual-manual modality to convey meaning. Sign languages are expressed through manual articulations in combination with non-manual elements. Sign languages are full-fledged natural languages with their own grammar and lexicon. Unlike acoustically conveyed sound patterns, sign language uses body language and manual communication to convey the thoughts of a person fluently. It is achieved by simultaneously combining hand shapes, orientation and movement of the hands, arms or body, and facial expressions. This way of communication is, however, limited to a group of people, which introduces a communication barrier. The need for an interpreter is essential to tackle this problem. A possible approach is using computer vision and machine learning techniques. This approach can, however, be costly and needs a bulky hardware setup or particular application installation. Also, it is subject to lighting conditions and perspective problems. We devised a cost-effective and easy-to-use glove that can overcome these limitations and act as an interpreter flawlessly.\nContributions Aimed at providing aid for speech-impaired people. Fabricated an easy-to-use, low-cost, and lightweight device in the form of a glove. ","date":1554076800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554076800,"objectID":"264e29e0cf3a67cb3354f66cc88418dc","permalink":"https://khush3.github.io/project/sign-language-detector/","publishdate":"2019-04-01T00:00:00Z","relpermalink":"/project/sign-language-detector/","section":"project","summary":"Wearable device to aid speech-impaired people.","tags":["Electronics","Embedded systems"],"title":"Sign Language Translator","type":"project"},{"authors":null,"categories":null,"content":"Abstract Robot localization is the process of determining where a mobile robot is located with respect to its environment. Localization is one of the most fundamental competencies required by an autonomous robot as the knowledge of the robot\u0026rsquo;s own location is an essential precursor to making decisions about future actions. In the summer of my freshman year, I worked on developing an algorithm to localize a differential-drive robot using odometry from scratch. I performed UMBmark test to calibrate robot base and wheel diameter constants. Besides this, I also developed the hardware for the robot.\nContributions Designed algorithm for pose (rectangular coordinates, angle) estimation of a robot in a two dimensional plane using odometry, and developed the hardware for robot. Used ROS framework to establish communication between the nodes. Performed UMBmark test to calibrate robot base and wheel diameter constants. Results MEASURED DATA REAL-TIME DATA ACCURACY ABSCISSA 25.4 cm 25.2 cm 99.3% ORDINATE 8.3 cm 8.3 cm 100% ANGLE 357.1 degree 356 degree 99.7% ","date":1525132800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1525132800,"objectID":"be255026871ce33ac491c2a8481c263c","permalink":"https://khush3.github.io/project/pose-estimating-mobile-robot/","publishdate":"2018-05-01T00:00:00Z","relpermalink":"/project/pose-estimating-mobile-robot/","section":"project","summary":"Algorithm and Hardware for 2-D pose estimation.","tags":["Electronics","Embedded systems","Robotics"],"title":"2-Dimensional Localization","type":"project"},{"authors":null,"categories":null,"content":"Abstract Simple harmonic motion can serve as a mathematical model for a variety of motions, such as the oscillation of a spring. Intending to learn computer vision and MATLAB, I worked on analyzing the motion of a target-object undergoing a damped harmonic motion. The target-object was separated from the background using color thresholding and estimated as a point object. Coordinates of this point were recorded and used to estimate the parameters associated with the mathematical model of the system like maximum displacement, mean position, the velocity at different time instants. A mathematical model was estimated by fitting a curve to the recorded data using MATLAB Curve Fitting Toolbox.\nContributions Estimated the equation of motion of a target-object video is selected to analyze its motion. Retrieved data associated with the motion of target-object using elementary mathematics. This project was made for TechnoSeason, 2017 and was awarded first prize. ","date":1525132800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1525132800,"objectID":"a79cee57d233b9013122c54e5a2dc3dc","permalink":"https://khush3.github.io/project/harmonic-motion-analyzer/","publishdate":"2018-05-01T00:00:00Z","relpermalink":"/project/harmonic-motion-analyzer/","section":"project","summary":"MATLAB program to analyze motion of a target object.","tags":["Computer vision","Image processing"],"title":"Harmonic Motion Analyzer","type":"project"}]